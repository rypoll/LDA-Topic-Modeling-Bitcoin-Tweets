\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{Abstract}{2}{section*.2}\protected@file@percent }
\newlabel{abstract}{{}{2}{Abstract}{section*.2}{}}
\@writefile{toc}{\contentsline {section}{Structure of the document}{2}{section*.3}\protected@file@percent }
\newlabel{structure-of-the-document}{{}{2}{Structure of the document}{section*.3}{}}
\@writefile{toc}{\contentsline {section}{1 Text Analysis and NLP: A Background}{2}{section*.4}\protected@file@percent }
\newlabel{text-analysis-and-nlp-a-background}{{}{2}{1 Text Analysis and NLP: A Background}{section*.4}{}}
\@writefile{toc}{\contentsline {subsection}{1.2 Sentiment Analysis}{3}{section*.5}\protected@file@percent }
\newlabel{sentiment-analysis}{{}{3}{1.2 Sentiment Analysis}{section*.5}{}}
\@writefile{toc}{\contentsline {subsection}{1.3 Supervised and Unsupervised Machine Learning with NLP}{4}{section*.6}\protected@file@percent }
\newlabel{supervised-and-unsupervised-machine-learning-with-nlp}{{}{4}{1.3 Supervised and Unsupervised Machine Learning with NLP}{section*.6}{}}
\@writefile{toc}{\contentsline {subsection}{1.4 Twitter's impact on the stock market}{4}{section*.7}\protected@file@percent }
\newlabel{twitters-impact-on-the-stock-market}{{}{4}{1.4 Twitter's impact on the stock market}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{1.5 How computers use text data}{4}{section*.8}\protected@file@percent }
\newlabel{how-computers-use-text-data}{{}{4}{1.5 How computers use text data}{section*.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A toy example of a document-term matrix which is part of the pre-processing step in order to perform NLP.}}{5}{figure.1}\protected@file@percent }
\newlabel{fig:bowmat}{{1}{5}{A toy example of a document-term matrix which is part of the pre-processing step in order to perform NLP}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{1.6 Applications of NLP}{5}{section*.9}\protected@file@percent }
\newlabel{applications-of-nlp}{{}{5}{1.6 Applications of NLP}{section*.9}{}}
\@writefile{toc}{\contentsline {section}{2 Aims}{7}{section*.10}\protected@file@percent }
\newlabel{aims}{{}{7}{2 Aims}{section*.10}{}}
\@writefile{toc}{\contentsline {subsection}{2.1 Motivation for and summary of the project}{7}{section*.11}\protected@file@percent }
\newlabel{motivation-for-and-summary-of-the-project}{{}{7}{2.1 Motivation for and summary of the project}{section*.11}{}}
\@writefile{toc}{\contentsline {subsection}{2.2 An overview of the approach to be taken}{7}{section*.12}\protected@file@percent }
\newlabel{an-overview-of-the-approach-to-be-taken}{{}{7}{2.2 An overview of the approach to be taken}{section*.12}{}}
\@writefile{toc}{\contentsline {subsection}{2.3 The Data}{8}{section*.13}\protected@file@percent }
\newlabel{the-data}{{}{8}{2.3 The Data}{section*.13}{}}
\@writefile{toc}{\contentsline {section}{3 Topic Models}{9}{section*.14}\protected@file@percent }
\newlabel{topic-models}{{}{9}{3 Topic Models}{section*.14}{}}
\@writefile{toc}{\contentsline {subsection}{3.1 Introduction}{9}{section*.15}\protected@file@percent }
\newlabel{introduction}{{}{9}{3.1 Introduction}{section*.15}{}}
\@writefile{toc}{\contentsline {subsection}{3.2 Methods for topic modeling}{9}{section*.16}\protected@file@percent }
\newlabel{methods-for-topic-modeling}{{}{9}{3.2 Methods for topic modeling}{section*.16}{}}
\@writefile{toc}{\contentsline {subsection}{3.3 Latent Semantic Indexing (LSA)}{9}{section*.17}\protected@file@percent }
\newlabel{latent-semantic-indexing-lsa}{{}{9}{3.3 Latent Semantic Indexing (LSA)}{section*.17}{}}
\@writefile{toc}{\contentsline {subsection}{3.4 Non-negative matrix factorisation}{9}{section*.18}\protected@file@percent }
\newlabel{non-negative-matrix-factorisation}{{}{9}{3.4 Non-negative matrix factorisation}{section*.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Singular Value Decomposition of the document-term matrix used in Latent Semantic Indexing to perform Topic Modeling.}}{10}{figure.2}\protected@file@percent }
\newlabel{fig:lsa_svd}{{2}{10}{Singular Value Decomposition of the document-term matrix used in Latent Semantic Indexing to perform Topic Modeling}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Non-negative matrix factorisation of the document-term matrix, a technique used to perform Topic Modeling.}}{10}{figure.3}\protected@file@percent }
\newlabel{fig:nmf}{{3}{10}{Non-negative matrix factorisation of the document-term matrix, a technique used to perform Topic Modeling}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{4 Latent Dirichlet Allocation}{11}{section*.19}\protected@file@percent }
\newlabel{latent-dirichlet-allocation}{{}{11}{4 Latent Dirichlet Allocation}{section*.19}{}}
\@writefile{toc}{\contentsline {subsection}{4.1 LDA Introduction}{11}{section*.20}\protected@file@percent }
\newlabel{lda-introduction}{{}{11}{4.1 LDA Introduction}{section*.20}{}}
\@writefile{toc}{\contentsline {subsection}{4.2 LDA In-Depth}{11}{section*.21}\protected@file@percent }
\newlabel{lda-in-depth}{{}{11}{4.2 LDA In-Depth}{section*.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Example of a multinomial distribution of the Topics in LDA for a single document.}}{11}{figure.4}\protected@file@percent }
\newlabel{fig:plot1}{{4}{11}{Example of a multinomial distribution of the Topics in LDA for a single document}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example of a multinomial distribution of the words in a Topic in LDA.}}{12}{figure.5}\protected@file@percent }
\newlabel{fig:plot2}{{5}{12}{Example of a multinomial distribution of the words in a Topic in LDA}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{4.2.1 The Generative Process of LDA}{12}{section*.22}\protected@file@percent }
\newlabel{the-generative-process-of-lda}{{}{12}{4.2.1 The Generative Process of LDA}{section*.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{4.2.1 Notation}{13}{section*.23}\protected@file@percent }
\newlabel{notation}{{}{13}{4.2.1 Notation}{section*.23}{}}
\@writefile{toc}{\contentsline {subsection}{4.3 The Dirichlet Distribution}{13}{section*.24}\protected@file@percent }
\newlabel{the-dirichlet-distribution}{{}{13}{4.3 The Dirichlet Distribution}{section*.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{4.3.1 Toy Example for the Dirichlet Distribution}{13}{section*.25}\protected@file@percent }
\newlabel{toy-example-for-the-dirichlet-distribution}{{}{13}{4.3.1 Toy Example for the Dirichlet Distribution}{section*.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The beta distribution with different parameters, which is the univariate version of the Dirichlet distriubtion used in the LDA Topic Modeling algorithm.}}{14}{figure.6}\protected@file@percent }
\newlabel{fig:betadist}{{6}{14}{The beta distribution with different parameters, which is the univariate version of the Dirichlet distriubtion used in the LDA Topic Modeling algorithm}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{4.3.2 Dirichlet in detail}{14}{section*.26}\protected@file@percent }
\newlabel{dirichlet-in-detail}{{}{14}{4.3.2 Dirichlet in detail}{section*.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces How the probabiliy distribution for the Dirichlet changes with varying values of alpha - one of the hyperparameters of the LDA algorithm. This hyperparamer influences the distribution of the multinomial distrubtion of topics for the documents, and the multinomial distribution of the words in each topic.}}{15}{figure.7}\protected@file@percent }
\newlabel{fig:dirivis2}{{7}{15}{How the probabiliy distribution for the Dirichlet changes with varying values of alpha - one of the hyperparameters of the LDA algorithm. This hyperparamer influences the distribution of the multinomial distrubtion of topics for the documents, and the multinomial distribution of the words in each topic}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{4.4 Plate notation}{16}{section*.27}\protected@file@percent }
\newlabel{plate-notation}{{}{16}{4.4 Plate notation}{section*.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Box diagram describing the generative process of LDA, showing the relationships between the latent variables, hyperparameters and distributions.}}{16}{figure.8}\protected@file@percent }
\newlabel{fig:boxdia}{{8}{16}{Box diagram describing the generative process of LDA, showing the relationships between the latent variables, hyperparameters and distributions}{figure.8}{}}
\@writefile{toc}{\contentsline {subsection}{4.5 Gibbs sampling}{16}{section*.28}\protected@file@percent }
\newlabel{gibbs-sampling}{{}{16}{4.5 Gibbs sampling}{section*.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Box diagram with additional notes giving more detail on how the LDA algorithm works.}}{17}{figure.9}\protected@file@percent }
\newlabel{fig:platedialab}{{9}{17}{Box diagram with additional notes giving more detail on how the LDA algorithm works}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{4.5.1 Gibbs Sampling in detail}{17}{section*.29}\protected@file@percent }
\newlabel{gibbs-sampling-in-detail}{{}{17}{4.5.1 Gibbs Sampling in detail}{section*.29}{}}
\@writefile{toc}{\contentsline {subsubsection}{Obtaining the topic and word distributions}{18}{section*.30}\protected@file@percent }
\newlabel{obtaining-the-topic-and-word-distributions}{{}{18}{Obtaining the topic and word distributions}{section*.30}{}}
\@writefile{toc}{\contentsline {subsection}{4.6 Benefits of LDA}{19}{section*.31}\protected@file@percent }
\newlabel{benefits-of-lda}{{}{19}{4.6 Benefits of LDA}{section*.31}{}}
\@writefile{toc}{\contentsline {section}{5 Pipeline Architecture}{20}{section*.32}\protected@file@percent }
\newlabel{pipeline-architecture}{{}{20}{5 Pipeline Architecture}{section*.32}{}}
\@writefile{toc}{\contentsline {subsection}{5.1 Pre-processing}{20}{section*.33}\protected@file@percent }
\newlabel{pre-processing}{{}{20}{5.1 Pre-processing}{section*.33}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.2 Lemmatisation}{22}{section*.34}\protected@file@percent }
\newlabel{lemmatisation}{{}{22}{5.1.2 Lemmatisation}{section*.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.3 Tokenisation}{22}{section*.35}\protected@file@percent }
\newlabel{tokenisation}{{}{22}{5.1.3 Tokenisation}{section*.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.4 n-grams: Including phrases}{22}{section*.36}\protected@file@percent }
\newlabel{n-grams-including-phrases}{{}{22}{5.1.4 n-grams: Including phrases}{section*.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.1.5 Filtering tokens that exhibit extreme frequencies}{22}{section*.37}\protected@file@percent }
\newlabel{filtering-tokens-that-exhibit-extreme-frequencies}{{}{22}{5.1.5 Filtering tokens that exhibit extreme frequencies}{section*.37}{}}
\@writefile{toc}{\contentsline {subsection}{5.2 Running the LDA Algorithm}{22}{section*.38}\protected@file@percent }
\newlabel{running-the-lda-algorithm}{{}{22}{5.2 Running the LDA Algorithm}{section*.38}{}}
\newlabel{inputs-into-the-model-hyperparameters}{{}{22}{5.2.1 Inputs into the model: Hyperparameters}{section*.39}{}}
\@writefile{toc}{\contentsline {paragraph}{5.2.1 Inputs into the model: Hyperparameters}{22}{section*.39}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Plot of how different optimisation intervals, which controls the optimisation of the hyperparameters, used in the LDA algorithm impacts the topic probability distribution. Courtesy of Christof Schöch (2016).}}{23}{figure.10}\protected@file@percent }
\newlabel{fig:optint}{{10}{23}{Plot of how different optimisation intervals, which controls the optimisation of the hyperparameters, used in the LDA algorithm impacts the topic probability distribution. Courtesy of Christof Schöch (2016)}{figure.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.2.1 Inputs into the model: Evaluating the Best Number of Topics}{23}{section*.40}\protected@file@percent }
\newlabel{inputs-into-the-model-evaluating-the-best-number-of-topics}{{}{23}{5.2.1 Inputs into the model: Evaluating the Best Number of Topics}{section*.40}{}}
\@writefile{toc}{\contentsline {subsubsection}{5.2.3 Running the Model}{24}{section*.41}\protected@file@percent }
\newlabel{running-the-model}{{}{24}{5.2.3 Running the Model}{section*.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Plot of UMass Coherence score over the number of topics outputted}}{25}{figure.11}\protected@file@percent }
\newlabel{fig:cohumass}{{11}{25}{Plot of UMass Coherence score over the number of topics outputted}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Plot of UMass Coherence score over the number of topics outputted}}{25}{figure.12}\protected@file@percent }
\newlabel{fig:cohcv}{{12}{25}{Plot of UMass Coherence score over the number of topics outputted}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{6 Visualisation}{27}{section*.42}\protected@file@percent }
\newlabel{visualisation}{{}{27}{6 Visualisation}{section*.42}{}}
\@writefile{toc}{\contentsline {subsection}{6.1 LDAvis}{27}{section*.43}\protected@file@percent }
\newlabel{ldavis}{{}{27}{6.1 LDAvis}{section*.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Output of LDAvis using 15 topic models for the Bitcoin data}}{27}{figure.13}\protected@file@percent }
\newlabel{fig:ldavis1}{{13}{27}{Output of LDAvis using 15 topic models for the Bitcoin data}{figure.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{6.1.1 ``Useful'' words}{27}{section*.44}\protected@file@percent }
\newlabel{useful-words}{{}{27}{6.1.1 ``Useful'' words}{section*.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The word relevance of Topic 7 in the LDAvis and setting lambda = 0.6}}{28}{figure.14}\protected@file@percent }
\newlabel{fig:ldavis2}{{14}{28}{The word relevance of Topic 7 in the LDAvis and setting lambda = 0.6}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{6.2 t-SNE}{28}{section*.45}\protected@file@percent }
\newlabel{t-sne}{{}{28}{6.2 t-SNE}{section*.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The t-SNE plot for the topic model for the Bitcoin data using 15 topics}}{29}{figure.15}\protected@file@percent }
\newlabel{fig:tsne3}{{15}{29}{The t-SNE plot for the topic model for the Bitcoin data using 15 topics}{figure.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{6.2 Using t-SNE to detect bot-generated Tweets}{29}{section*.46}\protected@file@percent }
\newlabel{using-t-sne-to-detect-bot-generated-tweets}{{}{29}{6.2 Using t-SNE to detect bot-generated Tweets}{section*.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Demonstration of how the t-SNE plot can identify Tweets produced by bots with the red box showing a cluster of similar Tweets. Tweets produced by bots are repeated and have the same form, therefore in the t-SNE plot they appear as their own cluster, since local structure is preserved. Hovering over these points with the mouse shows the content of these tweets.}}{30}{figure.16}\protected@file@percent }
\newlabel{fig:tsnebot}{{16}{30}{Demonstration of how the t-SNE plot can identify Tweets produced by bots with the red box showing a cluster of similar Tweets. Tweets produced by bots are repeated and have the same form, therefore in the t-SNE plot they appear as their own cluster, since local structure is preserved. Hovering