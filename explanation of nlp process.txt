# Process 

The tweets were obtained using Twitter's API, pulling tweets in the years of 2016 to 2021 inclusive. In order to collect tweets that involved Bitcoin, a search criteria was used using the following terms:

* "Bitcoin" 
* OR "BTC"
* OR "$BTC"

where "OR" is the logical operator that allows for any of the terms to appear in the tweet in order for it be extracted.

The term "BTC" is an abbreviation for bitcoin commonly used in the cryptocurrency market. The term "$BTC" is used to capture tweets that use "cash-tags", that is a method to tag a tweet using the dollar sign ($) in that users employ in order to tag their tweet.


## Cleaning 
Before an LDA model can be built, the tweets must be cleaned. This process is done to remove noise from the tweets so that the model can uncover the meaning of the tweets more effectively.

There are two main cleaning considerations when dealing with tweets.

1. The first is some tweets will not be relevant to our analysis. Examining the tweets uncovered a portion of tweets that don't express opinions of people and are most likely tweets by bots. Here is an example tweet

"Current price: 433.54$ $BTCUSD $btc #bitcoin 2016-01-02 18:40:05 EST"

This type of tweet appears multiple time in the dataset and more than likely is produced by a bot that serves to update users of twitter the price of bitcoin. There are various tweets such as these that can judged as being produced by bots and removing them from the data is important because they don't represent any sentiment of people. 

(can go into all filters here)

The removal of these tweets means that the sentiment of the users of Twitter can be more effectively investigated.


2.  The second consideration is noise within each tweet. For example, a word which appears in the data may be presented like this "#hate". This phrase carries the same meaning as the word hate phrased as this "hate". In this example, the hashtag is removed from "#hate" and the word is saved in the tweet as "hate".

This means that then for the statistical analysis, "#hate" and "hate" can be grouped together because they are exactly the same, and this is important because topic modelling's purpose is to group tweets based on similarities within those tweets and the way to pick up that similarity is by seeing if tweets have the same words inside them. The algorithm has no way of knowing if "#hate" and "hate" are the same word. For that characters such as hashtags are removed from the tweets, amongst other characters that will be explained further.

In addition to removal of special characters to ensure words with the same meaning are the same, the syntax of a word is cleaned. A short example of this is, the phrase "buy" and "bought" carry the same meaning - they just refer to different moments of time, the present and past respectively. For that reasons these words should appear the same for the algorithm; they will be both classed as the present "buy". This helps the algorithm give the same meaning to tweets that use words that only differ by their grammar (such as past, present or future).


After some cleaning and manipulation a dictionary can be produced that is then used in the LDA model. A dictionary is the list of all words in the corpus, where the corpus is all the tweets in the data. 



## The cleaning process in detail

1. All characters are converted to their lowercase version
This is done to group up the same words that may be expressed in different cases

2.  Tweets that aren't relevant are removed.

As mentioned in step 1 previously, tweets that are deemed irrelevant are removed. These tweets were deemed to be done by bots by running the topic model algorithm and exploring the position of these tweets on a plot (t-SNE plot which will be explored further on in the report). This is done by removing any tweets with the following terms:

		+ "current price"
		+ "current rate"
		+ "volume alert"
		+ "price update"
		+ "prices update"
		+ "price action"
		+ "price increase"
		+ "price decrease"
		+ "volume"
		+ "density"
		+ "last hour"
		+ "latest block info"
		+ "closed sell"
		+ "alert"
		+ "hourly update"
		+ "%" 
		+ If a tweet contains the words "current" and "price" in the same tweet in any position, it is removed.
		+ An additional step is taken here by removing tweets that have a lot of  numbers characters in the tweet. Some tweets by bots demonstrate a high amount  of numbers in their tweets, and an analysis was done to find the amount of numbers present in a tweet at which it should be removed - that is the amount of nubmer characters that should be removed that maxmises the removal of amount of tweets made by a bot and minises the removal of tweets made by humans. This number was found to be 10, therefore tweets with 10 or more numbers are removed.
Although not all tweets generated by bots cannot be guaranteed to be removed using this list, from inspection, the majority of bot tweets are removed.

3. Normalisation : removal of special characters INCLUDE THIS IN THE CODE
		+ convert html escapes like &amp; to characters.
		+ remove html tags like <tab>
		+ remove markdown URLs like [Some text](https://....)
		+ remove text or code in brackets like [0]
		+ remove standalone sequences of specials, matches &# but not #cool
		+ remove standalone sequences of hyphens like --- or ==
		+ remove sequences of white spaces
		+ remove hash signs (#)
		+ remove dollar signs ($)
		+ Normalize words in text that have been split across lines by a hyphen for visual consistency (aka hyphenated) by joining the pieces back together, without hyphen and whitespace.
		+ Normalize all “fancy” single- and double-quotation marks in text to just the basic ASCII equivalents
		+ Normalize unicode characters in text into canonical forms.
		+ Remove accents from any accented unicode characters in text, either by replacing them with ASCII equivalents or removing them entirely

4. Remove words that aren't useful to the analysis. These are words that are common in the data but don't express any topic within the topic of bitcoin

	+ Remove 'btc'
	+ Remove 'bitcoin'
	+ Remove 'crypto' 
	+ Remove 'cryptocurrency'
	+ Remove 'coin'
	
5. Remove "Stop words"

These are commonly used words in English that don't carry little to no information. Words such as "The" or "a" are filtered out. This removes noise from the data in order for the algorithm to give more focus to the more meaningful words.

6. Lemmatisation

This process makes words such as "closed", "closing", "closes" all the same. The tense doesn't tell us anything about the meaning of the word so we make these words the same as they have the same meaning.

This is useful in the analysis because we want to group all types of the word "close", be it in the past, or the gerund (an example of gerund in this case is "closing" - the "ing" version of words) into one lemma because they all have the same meaning.

7. Tokenisation

This process is the separation of the tweet into single words, so the LDA algorithm can determine where each word starts and ends. This is necessary for the algorithm to analyse the data

8. n-gram: Including phrases

This joins two or three (bigram or trigram, respectively) tokens together to form a two or three token phrase. If a two or three word phrase appears of often in the tweets, specifically if they appear more than 30 times, they are included in the dictionary which is used in the LDA algorithm.




9. Filter extreme frequencies

If a word appears only 5 times in the whole corpus, it is removed from the dictionary. If a word appears in eighty percent or more of the tweets, it is also removed from the dictionary. Since these words are removed, they are then not used in the LDA algorithm.

Words that are very frequent or not very frequent are not relevant to the analysis due to their frequency.


10. Create a dictionary

This is the list of all the words in the corpus (all the tweets).


11. Change the corpus into Bag of Words

This is a gives a unique number id to each word in each tweet, accompanied with the frequency of that word in the tweet. The algorithm will process the tweets using  the numerical representaiton of words and their frequencies. 



 Using a package called gensim and LDA Mallet, the model is run using the pre-processed data and using a beta parameter of 0.01 and an alpha parameter of 5/number of topics. 




## Evaluatiing the Best Number of Topics


The LDA model requires that the statistician inputs the number of topics that the corpus should be split into. Since the actual number of topics is hard to discern from such a large corpus of tweets, a different way to acertain nthe number of topics is necessary. Topic models make no guarantee on the inrepretability of their output nor their topics. 

The overall aim of evaluating a topic model is seeing if the outputted topics tell a good story. One way of doing that is called a "Word Intrusion" test. This is a test based on a human observers. 
For each topic the list of the most probably words in that topic are presented to an observer with a word that has a high-probability from another topic.

Original Topic: car, boot, window, drive, 
Original Topic with Word Intrusion: car, boot, banana (word intruder), window, drive
 
If the human observer can identify the word intruder, then the original makes sense and therefore is a good topic. If the observer can't identify the word then the original topic doesn't make sense because it means the word intruder looks as good as the words in the topic before. If this is the case, it suggests that the number of topics the user chose as a parameter should be changed.


This Word-Instrusion test is a measure of topic "Coherence". Coherence in this context is a measure of how much the documents support each other - or in other words how well the topics resemble an actual topic that human labels it as a distinct topic. These human-based measures serve as a gold standard for coherence evulation. However, they are expensve to produce and therefore a statistical based measure is called for. 

There are several popular coherence measures with UMass being one of them. This measure correlates well the the human based measure. It takes the set of N top words of a topic and sums a confirmation measure over all the word pairs in the set. A confirmation measure takes a pair of words or word subsets as well as the correspondin probability to compute how strong one set of words supports the other.  This confirmation measure is given by

Input equation 4 from https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf 
page 2

can also set up the example.



A paper (above) compared many different conherence measures by looking at their correlations with human based measures of topic models. The best performing coherence measure (the most left column) was found to be newly proposed measure. This measure (CV ) combines the indirect cosine measure with the NPMI and the boolean sliding window. See https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf
for more detail.

Since for each model the number of topics can be chosen, a model was run for each selected number of topics, that is the number in the set 5 to 40. The model that yields the highest coherence score highlights the model with the ideal number of topics; although it is important that the topic makes sense for a human and therefore these topics were analysed such that words in the topics were judged by the human to belong together.



## Visualisation 

Visualisig the result of a topic model is difficult since LDA since the fitted model has many dimensions - the LDA model is applied to thousands of documents, modeled as many topics, and these topics are modelled as distributions over many words. Using a pakacged called pyLDAvis, the result of the topic model can be visualised in a user-friendly way.

The left pane of the visualisation shows the whole view of the topic model, showing how prevalent each topic is and how the topics relate to each other. Each topic is a circle in a two-dimensional plane. Their positions are determined by computing the distance between topics and also using multidimensional scaling to project the inter-topic distances onto two dimensions (done in (Chuang et al., 2012a)). The prevalence of a topic is represented by the area of its circle. 

The right hand panel shows a bar chart, showing the most "useful" words for a topic highlighted on the left panel. The intends to show the meaning of the topic. The bars show two overlaid bars per word - the topic-specific frequency of a word (red) and the corpus-wide frequency of the word (blue). Selecting a word on the right changes the size of circles of the topics on the left - giving the conditional distribution over topics of the chosen word. 


### "Useful" words

In order to observe a topic meaning, a ranked list of the most probable terms in that topic. The issue with investigating topics like this is that common words in the corpus occur in the ranked list of multiple topics which in turn makes it hard to differentiate meanings of these topics. In order to resolve this issue Bischof and Airoldi (2012) suggested ranking terms for a given topic by using the frequency of the term in that topic and the exclusitivty of the term to the topic. The exclusivitiy takes into account how many times the term appears in that topic to the exclusion of others. This metric is defined as "usefulness" and pyLDAvis uses a similar measure called "relevance" to rank terms in order of "usefulness". 

define relevance the formula

https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf conducted a user study to determine whether there was an optimal value of lamba from the definition of relevance. 


pyLDAvis







DONT USE
A high UMass measure would indicate that the topics are well defined.

Another coherence measure was proposed  https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf which is a unifying framework which combines different coherence measures into one score. It works as follows.

1. **S - Segmentation** : Divide a word set (such as the top most probable words in a topic) into smaller pieces, such as a set of pairs. This is a set of different kinds of segmentations
2. **M - Confirmation Measure** : The set of conffirmation measure that scores the agreement of a given pair, for example UMass 
3. **P - Word probabilities** : These can be computed i ndifferent ways so P serves as a set of ways the word probabilities are calculated.
4. **Epsilon - Aggregate**: This is a set of aggregration functions which determine how to aggregate scaalr values.

DONT USE END






There are several methods to use statistical measures to determine the ideal number of topics. One such method is known as the Coherence Score, where a high score represesents.

To find the best Coherence Scrore, a model was built for each n, number of topics. For example, to investigate the best nubmef of topics, a model is first built using the parameter n = 3, 3 topics, and the coherence score is obtained. This process is repeated for n = 4, up until n = 30 and the highest coherence score amongst these models indicates the ideal number of topics. 

Cv has issues,  as stated herehttps://palmetto.demos.dice-research.org/
2 issues
https://github.com/dice-group/Palmetto/issues/13


i think umass doesnt use wikipedia, but hte rest doesnt. so that makes me think dont use the wikipedia one because these topics are so granular
http://qpleple.com/topic-coherence-to-evaluate-topic-models/



https://stats.stackexchange.com/questions/375062/how-does-topic-coherence-score-in-lda-intuitively-makes-sense 
more explanation







	




